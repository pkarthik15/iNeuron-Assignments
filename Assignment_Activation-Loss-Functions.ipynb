{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid is a non linear actiation function looks like a S-shape function\n",
    "\n",
    "![title](images/sigmoid.png)\n",
    "\n",
    "1. The output of sigmoid activation function is between 0 and 1. \n",
    "2. Usually used in output layer of a binary classification, where result is either 0 or 1, as value for sigmoid function lies between 0 and 1 only so, result can be predicted easily to be 1 if value is greater than 0.5 and 0 otherwise.\n",
    "3. Smooth gradient, preventing “jumps” in output values\n",
    "4. Clear predictions—For X above 2 or below -2, tends to bring the Y value (the prediction) to the edge of the curve, very close to 1 or 0. This enables clear predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tanh — Hyperbolic tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation that works almost always better than sigmoid function is Tanh function also knows as Tangent Hyperbolic function. It’s actually mathematically shifted version of the sigmoid function. Both are similar and can be derived from each other.\n",
    "\n",
    "![title](images/tanh.png)\n",
    "\n",
    "1. Value Range :- -1 to +1\n",
    "2. Nature :- non-linear\n",
    "3. Usually used in hidden layers of a neural network as it’s values lies between -1 to 1 hence the mean for the hidden layer comes out be 0 or very close to it, hence helps in centering the data by bringing mean close to 0. This makes learning for the next layer much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.\n",
    "\n",
    "![title](images/relu.png)\n",
    "\n",
    "As you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.\n",
    "\n",
    "1. Computationally efficient—allows the network to converge very quickly\n",
    "2. Non-linear—although it looks like a linear function, ReLU has a derivative function and allows for backpropagation\n",
    "3. ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. At a time only a few neurons are activated making the network sparse making it efficient and easy for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is also a type of sigmoid function but is handy when we are trying to handle classification problems.\n",
    "\n",
    "![title](images/softmax.jpg)\n",
    "\n",
    "only one class in other activation functions—normalizes the outputs for each class between 0 and 1, and divides by their sum, giving the probability of the input value being in a specific class.\n",
    "\n",
    "1. non-linear\n",
    "2. Usually used when trying to handle multiple classes. The softmax function would squeeze the outputs for each class between 0 and 1 and would also divide by the sum of the outputs.\n",
    "3. The softmax function is ideally used in the output layer of the classifier where we are actually trying to attain the probabilities to define the class of each input. \n",
    "4. Which is mostly used in multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so.\n",
    "Therefore the range of the Leaky ReLU is (-infinity to infinity).\n",
    "his variation of ReLU has a small positive slope in the negative area, so it does enable backpropagation, even for negative input values\n",
    "\n",
    "![title](images/leaky.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Step function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value of Y is above a certain value, declare it activated. If it’s less than the threshold, then say it’s not\n",
    "Activation function A = “activated” if Y > threshold else not\n",
    "Alternatively, A = 1 if y> threshold, 0 otherwise\n",
    "\n",
    "![title](images/step.png)\n",
    "\n",
    "Its output is 1 ( activated) when value > 0 (threshold) and outputs a 0 ( not activated) otherwise.\n",
    "\n",
    "Suppose you are creating a binary classifier. Something which should say a “yes” or “no” ( activate or not activate ). A Step function could do that for you! That’s exactly what it does, say a 1 or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Exponential Linear Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential Linear Unit or ELU for short is also a variant of Rectiufied Linear Unit (ReLU) that modifies the slope of the negative part of the function. Unlike the leaky relu and parametric ReLU functions, instead of a straight line, ELU uses a log curve for defning the negatice values. It is defined as\n",
    "\n",
    "f(x) = x,   x>=0\n",
    "\n",
    "     = a(e^x-1), x<0\n",
    "     \n",
    "![title](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/10/elu-and-relu-300x194.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swish is a lesser known activation function which was discovered by researchers at Google. Swish is as computationally efficient as ReLU and shows better performance than ReLU on deeper models.  The values for swish ranges from negative infinity to infinity. The function is defined as –\n",
    "\n",
    "f(x) = x*sigmoid(x)\n",
    "f(x) = x/(1-e^-x)\n",
    "\n",
    "![title](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/10/swish-1024x511-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw the problem with the step function, the gradient of the function became zero. This is because there is no component of x in the binary step function. Instead of a binary function, we can use a linear function. We can define the function as-\n",
    "\n",
    "f(x)=ax\n",
    "\n",
    "![title](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/10/17154239/linear-300x300.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Activation function comes from the special class of function known as radial basis functions (RBFs) are used in RBF networks\n",
    "\n",
    "These functions are Bell-Shaped Curves that comes with the properties of having continuous.\n",
    "\n",
    "The Output Node of the Gaussian Activation function is meant to be interpreted in terms of “1” or “0”, depending on how close the input is to a chosen value of average.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
    "\n",
    "![title](images/mse.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mean Absolute Error/L1 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. Like MSE, this as well measures the magnitude of error without considering their direction. Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
    "\n",
    "![title](images/mae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Binary Cross Entropy Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we use entropy to indicate disorder or uncertainty. It is measured for a random variable X with probability distribution p(X):\n",
    "\n",
    "![title](images/bce.jpg)\n",
    "\n",
    "![title](images/bce2jpg.jpg)\n",
    "\n",
    "A greater value of entropy for a probability distribution indicates a greater uncertainty in the distribution. Likewise, a smaller value indicates a more certain distribution\n",
    "\n",
    "This makes binary cross-entropy suitable as a loss function – you want to minimize its value. We use binary cross-entropy loss for classification models which output a probability p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinge loss is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1. So make sure you change the label of the ‘Malignant’ class in the dataset from 0 to -1.\n",
    "\n",
    "Hinge Loss not only penalizes the wrong predictions but also the right predictions that are not confident.\n",
    "\n",
    "![title](images/hinge.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multi-Class Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-class cross-entropy loss is a generalization of the Binary Cross Entropy loss. The loss for input vector X_i and the corresponding one-hot encoded target vector Y_i is:\n",
    "\n",
    "![title](images/mce.jpg)\n",
    "\n",
    "Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. KL-Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
    "\n",
    "A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
    "\n",
    "As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
    "\n",
    "![title](images/kld.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Huber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically used for regression. It’s less sensitive to outliers than the MSE as it treats error as square only inside an interval.\n",
    "\n",
    "![title](images/huber.png)\n",
    "\n",
    " It combines the best properties of L2 squared loss and L1 absolute loss by being strongly convex when close to the target/minimum and less steep for extreme values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. logcosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-cosh is another function used in regression tasks that’s smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.\n",
    "\n",
    "![title](images/logcosh.png)\n",
    "\n",
    "his means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. It has all the advantages of Huber loss, and it’s twice differentiable everywhere, unlike Huber loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poisson loss function is a measure of how the predicted distribution diverges from the expected distribution, the Poisson as loss function is a variant from Poisson Distribution, where the Poisson distribution is widely used for modeling count data. If we draw a sequence of n independent observations from a Poisson distribution then the Poisson Loss function ‘P’ is defined as the likelihood of observing this sequence for value of λ which maximizes this likelihood.\n",
    "\n",
    "![title](images/poisson.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Cosine Proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine_proximity loss computes the cosine proximity between the output of the network and the y values you supply to the fit function. Thus, with the setup you have here, it's computing the cosine between the output of your cosine-merge->relu with whatever your Y is.\n",
    "\n",
    "I think one option to do what it seems you are trying to do is have is a custom loss function that will result in minimizing the negative output of your network (and either disregards Y or uses Y as some margin-like value).\n",
    "\n",
    "As a side note, if your inputs are sentences from the same language, consider sharing your network that computes the representation using a network that is shared across your two inputs (e.g. siamese network).\n",
    "\n",
    "![title](images/cosine.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
